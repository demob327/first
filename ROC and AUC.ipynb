{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter NotebookActivity_4_ROC_curve_and_AUC (autosaved)Python 3 [3.6]\n",
    "Python 3 [3.6] \n",
    "File\n",
    "Edit\n",
    "View\n",
    "Insert\n",
    "Cell\n",
    "Kernel\n",
    "Widgets\n",
    "Help\n",
    "\n",
    "Code\n",
    "The ROC curve and the AUC\n",
    "Let's use the same three datasets we had earlier, and come up with our own code for constructing the ROC curve, i.e., calculate the confusion matrix for various thresholds.\n",
    "\n",
    "While this is out of scope for now, note that I now use linear regression to obtain scores, rather than nominal outcomes you get from applying logistic regression. It is useful to do the latter as well, and save them for comparison's sake. The difference will become apparent as you progress through this course.\n",
    "\n",
    "So, first, we have the data to obtain the actual outcomes and the predicted outcome:\n",
    "\n",
    "The data and models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve as roc\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "\n",
    "dat1 = make_classification(n_samples=100, \n",
    "                    n_features=3, \n",
    "                    n_informative=3, \n",
    "                    n_redundant=0, \n",
    "                    n_repeated=0, \n",
    "                    n_classes=2, \n",
    "                    n_clusters_per_class=2, random_state=8)\n",
    "dat2 = make_classification(n_samples=100, \n",
    "                    n_features=3, \n",
    "                    n_informative=3, \n",
    "                    n_redundant=0, \n",
    "                    n_repeated=0, \n",
    "                    n_classes=2, \n",
    "                    n_clusters_per_class=2, random_state=9)\n",
    "dat3 = make_classification(n_samples=100, \n",
    "                    n_features=3, \n",
    "                    n_informative=3, \n",
    "                    n_redundant=0, \n",
    "                    n_repeated=0, \n",
    "                    n_classes=2, \n",
    "                    n_clusters_per_class=2, random_state=6)\n",
    "\n",
    "# The continuous predictions from the linear regression\n",
    "lin_regr = LinearRegression(normalize=True)\n",
    "lin_regr.fit(dat1[0], dat1[1])\n",
    "output1 = lin_regr.predict(dat1[0])\n",
    "lin_regr.fit(dat2[0], dat2[1])\n",
    "output2 = lin_regr.predict(dat2[0])\n",
    "lin_regr.fit(dat2[0], dat2[1])\n",
    "output3 = lin_regr.predict(dat3[0])\n",
    "\n",
    "actuals = [dat1[1],dat2[1],dat3[1]]\n",
    "predicted = [output1,output2,output3]\n",
    "\n",
    "# The discrete predictions from the logistic regression\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg.fit(dat1[0],dat1[1])\n",
    "outputD1 = log_reg.predict(dat1[0])\n",
    "log_reg.fit(dat2[0],dat2[1])\n",
    "outputD2 = log_reg.predict(dat2[0])\n",
    "log_reg.fit(dat3[0],dat3[1])\n",
    "outputD3 = log_reg.predict(dat3[0])\n",
    "data = [outputD1,outputD2,outputD3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The metrics\n",
    "#We might find it interesting to use the methods from earlier as well:\n",
    "\n",
    "def calculate_accuracy(TP,FP,FN,TN):\n",
    "    return (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "def calculate_recall(TP,FP,FN,TN):\n",
    "    return (TP)/(TP+FN)    \n",
    "    \n",
    "def calculate_specificity(TP,FP,FN,TN):   \n",
    "    return (TN)/(TN+FP)  \n",
    "\n",
    "def calculate_precision(TP,FP,FN,TN):   \n",
    "    return (TP)/(TP+FP)  \n",
    "\n",
    "def calculate_fallout(TP,FP,FN,TN):   \n",
    "    return (FP)/(FP+TN)  \n",
    "\n",
    "def calculate_fscore(TP,FP,FN,TN):   \n",
    "    return 2/((1/calculate_recall(TP,FP,FN,TN))+(1/calculate_precision(TP,FP,FN,TN))) \n",
    "\n",
    "def calculate_cm(predicted, actual):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    for i in range(0,len(predicted)):\n",
    "        if predicted[i] == 1:\n",
    "            if predicted[i] == actual[i]:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if predicted[i] == actual[i]:\n",
    "                TN+=1\n",
    "            else:\n",
    "                FN+=1\n",
    "    return TP,FP,FN,TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the thresholds\n",
    "#We also need the thresholds for our curve. I will illustrate this quickly for dataset 1. Let's just use all values (scores) we observed in the dataset:\n",
    "\n",
    "# The set operation makes sure every value is unique. For convenience, it is immediately converted into a list.\n",
    "thresholds = set(predicted[0])\n",
    "thresholds = list(thresholds)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the ROC curve\n",
    "#Now let's write a function for calculating the confusion matrix at a certain threshold:\n",
    "\n",
    "def calculate_at_threshold(threshold, actual, predicted):\n",
    "    x_coord = 0\n",
    "    y_coord = 0\n",
    "    \n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    \n",
    "    return x_coord, y_coord \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Then, check your results:\n",
    "\n",
    "assert calculate_at_threshold(0.8, actuals[0],predicted[0]) == (0.0, 0.09803921568627451)\n",
    "\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting it all together\n",
    "#Now, let's bring it all together for the three datasets:\n",
    "\n",
    "for i in range(0,3):\n",
    "    \n",
    "    # Let's calculate the standard metrics for the logistic regression output\n",
    "    print('Dataset '+str(i))\n",
    "    TP,FP,FN,TN = calculate_cm(data[i], actuals[i])\n",
    "    print('\\tT\\tF')\n",
    "    print('T\\t'+str(TP)+'\\t'+str(FP))\n",
    "    print('F\\t'+str(FN)+'\\t'+str(TN))\n",
    "        \n",
    "    print('Accuracy '+str(calculate_accuracy(TP,FP,FN,TN)))\n",
    "    print('Recall '+str(calculate_recall(TP,FP,FN,TN)))\n",
    "    print('Precision '+str(calculate_precision(TP,FP,FN,TN)))\n",
    "    print('Specificity '+str(calculate_specificity(TP,FP,FN,TN)))\n",
    "    print('Fall-out '+str(calculate_fallout(TP,FP,FN,TN)))\n",
    "    print('F1-score '+str(calculate_fscore(TP,FP,FN,TN)))\n",
    "    \n",
    "    # Same as before\n",
    "    thresholds = set(predicted[i])\n",
    "    thresholds = list(thresholds)\n",
    "        \n",
    "    # Find all the coordinates (false and true positive rate on the x and y axis respectively) and save them in fpr and tpr\n",
    "    # For all thresholds\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    for threshold in thresholds:   \n",
    "        if threshold >= 0 and threshold<=1:\n",
    "            fpr_t, tpr_t = calculate_at_threshold(threshold,actuals[i],predicted[i])\n",
    "            fpr.append(fpr_t)\n",
    "            tpr.append(tpr_t)\n",
    "    fpr = np.asarray(fpr)\n",
    "    tpr = np.asarray(tpr)\n",
    "    \n",
    "    # Make sure (0,0) and (1,1) are included\n",
    "    fpr = np.append(fpr,0)\n",
    "    tpr = np.append(tpr,0)\n",
    "    fpr = np.append(fpr,1)\n",
    "    tpr = np.append(tpr,1)\n",
    "    \n",
    "    # This function sorts all points in the false positive rate dimension\n",
    "    desc_score_indices = np.argsort(fpr)[::-1]\n",
    "    fpr = fpr[desc_score_indices]\n",
    "    tpr = tpr[desc_score_indices]\n",
    "â€‹\n",
    "    \n",
    "    # This function calculates the area under curve\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    \n",
    "    # Create the graph \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.plot(fpr, tpr,\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()    \n",
    "        \n",
    "    # Or do it the easy way and calculate it with the roc() function\n",
    "    fpr, tpr, thresholds = roc(actuals[i],predicted[i])\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.plot(fpr, tpr,\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
